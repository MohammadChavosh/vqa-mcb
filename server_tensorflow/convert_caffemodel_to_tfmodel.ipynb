{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from __future__ import absolute_import, division, print_function\n",
    "\n",
    "import os; os.environ['CUDA_VISIBLE_DEVICES'] = '0'\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "sess = tf.Session(config=tf.ConfigProto(gpu_options=tf.GPUOptions(allow_growth=True)))\n",
    "\n",
    "# Our implementation of ConvNet layers and Compact Bilinear in TensorFlow\n",
    "from vqa_mcb_model import vqa_mcb_model\n",
    "\n",
    "# For ResNet, we (currently) still use Caffe to extract image features.\n",
    "# We DO NOT need the vqa Caffe branch. You can use standard Caffe for ResNet.\n",
    "import sys; sys.path.append('/home/ronghang/workspace/caffe-vqa-mcb/python')\n",
    "import caffe; caffe.set_mode_cpu()\n",
    "\n",
    "# For loading data, we use the LoadVQADataProvider in original code\n",
    "import vqa_data_provider_layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# constants\n",
    "MCB_PROTOTXT_PATH = '../data/multi_att_2_glove_pretrained/proto_test_batchsize1.prototxt'\n",
    "MCB_CAFFEMODEL_PATH =  '../data/multi_att_2_glove_pretrained/_iter_190000.caffemodel'\n",
    "vqa_data_provider_layer.CURRENT_DATA_SHAPE = (2048, 14, 14)\n",
    "\n",
    "# Converted from the corresponding Caffe model\n",
    "SAVE_MODEL = './tf_vqa_data/_iter_190000.tfmodel'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def convert_embed_param(params):\n",
    "    embed_W = params[0].data[...].copy()\n",
    "    embed_B = params[1].data[...].copy()\n",
    "    return embed_W, embed_B\n",
    "\n",
    "def convert_conv_param(params):\n",
    "    W = params[0].data.transpose((2, 3, 1, 0))\n",
    "    B = params[1].data[...].copy()\n",
    "    return W, B\n",
    "\n",
    "def convert_fc_param(params):\n",
    "    W = params[0].data.transpose((1, 0))\n",
    "    B = params[1].data[...].copy()\n",
    "    return W, B\n",
    "\n",
    "def convert_lstm_param(params):\n",
    "    W = np.hstack((params[0].data, params[2].data))  # input before states\n",
    "    B = params[1].data\n",
    "    \n",
    "    # convert the gate order\n",
    "    W_i, W_f, W_o, W_g = np.split(W, 4, axis=0)\n",
    "    W = np.vstack((W_i, W_g, W_f, W_o))\n",
    "    W = W.transpose((1, 0))\n",
    "    B_i, B_f, B_o, B_g = np.split(B, 4)\n",
    "    B = np.hstack((B_i, B_g, B_f, B_o))\n",
    "    return W, B\n",
    "\n",
    "def assign_var(name, value):\n",
    "    return tf.assign(tf.get_variable(name), value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "caffe_net = caffe.Net(MCB_PROTOTXT_PATH, MCB_CAFFEMODEL_PATH, caffe.TEST)\n",
    "\n",
    "embed_W, embed_B = convert_embed_param(caffe_net.params['embed_ba'])\n",
    "att_conv1_W, att_conv1_B = convert_conv_param(caffe_net.params['att_conv1'])\n",
    "att_conv2_W, att_conv2_B = convert_conv_param(caffe_net.params['att_conv2'])\n",
    "prediction_W, prediction_B = convert_fc_param(caffe_net.params['prediction'])\n",
    "lstm1_W, lstm1_B = convert_lstm_param(caffe_net.params['lstm1'])\n",
    "lstm2_W, lstm2_B = convert_lstm_param(caffe_net.params['lstm2'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "max_time = 20\n",
    "batch_size = 1\n",
    "glove_dim = 300\n",
    "feat_h, feat_w, img_feat_dim = 14, 14, 2048\n",
    "num_vocab = 21025\n",
    "lstm_output_dim = 1024\n",
    "lstm_layers = 2\n",
    "embed_dim = 300\n",
    "cbp0_dim= 16000\n",
    "cbp1_dim = 16000\n",
    "num_classes = 3000\n",
    "\n",
    "word_indices = tf.placeholder(tf.int32, [max_time, batch_size])\n",
    "glove_vector = tf.placeholder(tf.float32, [max_time, batch_size, glove_dim])\n",
    "seq_length = tf.placeholder(tf.int32, [batch_size])\n",
    "img_feature = tf.placeholder(tf.float32, [batch_size, feat_h, feat_w, img_feat_dim])\n",
    "prediction, att_softmax0, _ = vqa_mcb_model(word_indices, glove_vector,\n",
    "    seq_length, img_feature, batch_size, num_vocab, embed_dim, glove_dim, max_time,\n",
    "    lstm_output_dim, lstm_layers, feat_h, feat_w, img_feat_dim, cbp0_dim, cbp1_dim,\n",
    "    num_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "init_ops = []\n",
    "with tf.variable_scope('vqa_mcb', reuse=True):\n",
    "    init_ops.append(assign_var('embedding/weights', embed_W))\n",
    "    init_ops.append(assign_var('embedding/biases', embed_B))\n",
    "    init_ops.append(assign_var('lstm12/multi_rnn_cell/cell_0/basic_lstm_cell/weights', lstm1_W))\n",
    "    init_ops.append(assign_var('lstm12/multi_rnn_cell/cell_0/basic_lstm_cell/biases', lstm1_B))\n",
    "    init_ops.append(assign_var('lstm12/multi_rnn_cell/cell_1/basic_lstm_cell/weights', lstm2_W))\n",
    "    init_ops.append(assign_var('lstm12/multi_rnn_cell/cell_1/basic_lstm_cell/biases', lstm2_B))\n",
    "    init_ops.append(assign_var('att_conv1/weights', att_conv1_W))\n",
    "    init_ops.append(assign_var('att_conv1/biases', att_conv1_B))\n",
    "    init_ops.append(assign_var('att_conv2/weights', att_conv2_W))\n",
    "    init_ops.append(assign_var('att_conv2/biases', att_conv2_B))\n",
    "    init_ops.append(assign_var('prediction/weights', prediction_W))\n",
    "    init_ops.append(assign_var('prediction/biases', prediction_B))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "saver = tf.train.Saver(write_version=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:*******************************************************\n",
      "WARNING:tensorflow:TensorFlow's V1 checkpoint format has been deprecated.\n",
      "WARNING:tensorflow:Consider switching to the more efficient V2 format:\n",
      "WARNING:tensorflow:   `tf.train.Saver(write_version=tf.train.SaverDef.V2)`\n",
      "WARNING:tensorflow:now on by default.\n",
      "WARNING:tensorflow:*******************************************************\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'./tf_vqa_data/_iter_190000.tfmodel'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sess.run(tf.group(*init_ops))\n",
    "saver.save(sess, SAVE_MODEL, write_meta_graph=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
